# 2.4 Floating Point

A floating-point representation encodes rational numbers of the form V = $$x × 2^y$$. It is useful for performing computations involving very large numbers (|V| $$\gg$$ 0), numbers very close to 0 (|V| $$\ll$$ 1), and more generally as an approximation to real arithmetic.

Up until the 1980s, every computer manufacturer devised its own conventions for how floating-point numbers were represented and the details of the operations performed on them. In addition, they often did not worry too much about the accuracy of the operations, viewing speed and ease of implementation as being more critical than numerical precision.

All of this changed around 1985 with the advent of **IEEE Standard 754**, a carefully crafted standard for representing floating-point numbers and the operations performed on them. This effort started in 1976 under Intel’s sponsorship with the design of the 8087, a chip that provided floating-point support for the 8086 processor. Intel hired William Kahan, a professor at the University of California, Berkeley, as a consultant to help design a floating-point standard for its future processors. They allowed Kahan to join forces with a committee generating an industry-wide standard under the auspices of the Institute of Electrical and Electronics Engineers (IEEE). The committee ultimately adopted a standard close to the one Kahan had devised for Intel. Nowadays, virtually all computers support what has become known as IEEE floating point. This has greatly improved the portability of scientific application programs across different machines.

In this section, we will see how numbers are represented in the IEEE floatingpoint format. We will also explore issues of **rounding**, when a number cannot be represented exactly in the format and hence must be adjusted upward or downward. We will then explore the mathematical properties of addition, multiplication, and relational operators. Many programmers consider floating point to be at best uninteresting and at worst arcane and incomprehensible. We will see that since the IEEE format is based on a small and consistent set of principles, it is really quite elegant and understandable.

{% hint style="info" %}
#### Aside --- The IEEE

The Institute of Electrical and Electronics Engineers (IEEE — pronounced “eye-triple-ee”) is a professional society that encompasses all of electronic and computer technology. It publishes journals, sponsors conferences, and sets up committees to define standards on topics ranging from power transmission to software engineering. Another example of an IEEE standard is the 802.11 standard for wireless networking.
{% endhint %}

## 2.4.1 Fractional Binary Numbers

A first step in understanding floating-point numbers is to consider binary numbers having fractional values. Let us first examine the more familiar decimal notation. Decimal notation uses a representation of the form

$$
d_m d_{m−1} ... d_1 d_0 . d_{−1} d_{−2} ... d_{−n}
$$

where each decimal digit $$d_i$$ ranges between 0 and 9. This notation represents a value d defined as

$$
d = \sum ^m_{i=-n} 10^i \times d_i
$$

The weighting of the digits is defined relative to the **decimal point** symbol (‘.’), meaning that digits to the left are weighted by nonnegative powers of 10, giving integral values, while digits to the right are weighted by negative powers of 10, giving fractional values. For example, 12.3410 represents the number $$1 \times 10^1 + 2 \times 10^0 + 3 \times 10^{−1} + 4 \times 10^{−2} = 12 \frac{34}{100}$$.

By analogy, consider a notation of the form

$$
b_m b_{m−1} ... b_1 b_0 . b_{−1} b_{−2} ... b_{−n}
$$

where each binary digit, or bit, $$b_i$$ ranges between 0 and 1, as is illustrated in Figure 2.31. This notation represents a number b defined as

$$
b = \sum^m_{i=-n} 2^i \times b_i \tag{2.19}
$$

![Figure 2.31
Fractional binary representation.
Digits to the left of the binary point have weights of the form 2^i , while those to the right have weights of the form 1/2^i .](<../../.gitbook/assets/image (32).png>)

The symbol ‘.’ now becomes a binary point, with bits on the left being weighted by nonnegative powers of 2, and those on the right being weighted by negative powers of 2. For example, $$101.11_2$$ represents the number $$1 × 2^2 + 0 × 2^1 + 1 × 2^0 + 1 × 2^{−1} + 1 × 2^{−2} = 4 + 0 + 1 + \frac{1}{2} + \frac{1}{4} = 5\frac{3}{4}$$.

One can readily see from Equation 2.19 that shifting the binary point one position to the left has the effect of dividing the number by 2. For example, while 101.112 represents the number $$5\frac{3}{4}$$, $$10.111_2$$ represents the number 2 + 0 + 1/2 + 1/4 + 1/8 = 2$$\frac{7}{8}$$. Similarly, shifting the binary point one position to the right has the effect of multiplying the number by 2. For example, $$1011.1_2$$ represents the number 8 + 0 + 2 + 1 + 1/2 = 11$$\frac{1}{2}$$.

Note that numbers of the form $$0.11 ... 1_2$$ represent numbers just below 1. For example, $$0.111111_2$$ represents $$\frac{63}{64}$$. We will use the shorthand notation $$1.0 − \epsilon$$ to represent such values.

Assuming we consider only finite-length encodings, decimal notation cannot represent numbers such as 1/3 and 5/7 exactly. Similarly, fractional binary notation can only represent numbers that can be written $$x \times 2^y$$. Other values can only be approximated. For example, the number 1/5 can be represented exactly as the fractional decimal number 0.20. As a fractional binary number, however, we cannot represent it exactly and instead must approximate it with increasing accuracy by lengthening the binary representation:

![](<../../.gitbook/assets/image (9).png>)

{% tabs %}
{% tab title="Practice Problem 2.45" %}
Fill in the missing information in the following table:

| Fractional value | Binary representation | Decimal representation |
| ---------------- | --------------------- | ---------------------- |
| $$\frac{1}{8}$$  | 0.001                 | 0.125                  |
| $$\frac{3}{4}$$  |                       |                        |
| $$\frac{5}{16}$$ |                       |                        |
|                  | 10.1011               |                        |
|                  | 1.001                 |                        |
|                  |                       | 5.875                  |
|                  |                       | 3.1875                 |
{% endtab %}

{% tab title="Practice Problem 2.46" %}
The imprecision of floating-point arithmetic can have disastrous effects. On February 25, 1991, during the first Gulf War, an American Patriot Missile battery in Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud missile. The Scud struck an American Army barracks and killed 28 soldiers. The US General Accounting Office (GAO) conducted a detailed analysis of the failure \[76] and determined that the underlying cause was an imprecision in a numeric calculation. In this exercise, you will reproduce part of the GAO’s analysis.

The Patriot system contains an internal clock, implemented as a counter that is incremented every 0.1 seconds. To determine the time in seconds, the program would multiply the value of this counter by a 24-bit quantity that was a fractional binary approximation to $$\frac{1}{10}$$. In particular, the binary representation of $$\frac{1}{10}$$ is the nonterminating sequence $$0.000110011[0011]..._2$$, where the portion in brackets is repeated indefinitely. The program approximated 0.1, as a value x, by considering just the first 23 bits of the sequence to the right of the binary point: x = 0.00011001100110011001100. (See Problem 2.51 for a discussion of how they could have approximated 0.1 more precisely.)

A. What is the binary representation of 0.1 − x?

B. What is the approximate decimal value of 0.1 − x?

C. The clock starts at 0 when the system is first powered up and keeps counting up from there. In this case, the system had been running for around 100 hours. What was the difference between the actual time and the time computed by the software?

D. The system predicts where an incoming missile will appear based on its velocity and the time of the last radar detection. Given that a Scud travels at around 2,000 meters per second, how far off was its prediction?

Normally, a slight error in the absolute time reported by a clock reading would not affect a tracking computation. Instead, it should depend on the relative time between two successive readings. The problem was that the Patriot software had been upgraded to use a more accurate function for reading time, but not all of the function calls had been replaced by the new code. As a result, the tracking software used the accurate time for one reading and the inaccurate time for the other \[103].
{% endtab %}
{% endtabs %}



{% tabs %}
{% tab title="Solution 2.45" %}


| Fractional value    | Binary representation | Decimal representation |
| ------------------- | --------------------- | ---------------------- |
| $$\frac{1}{8}$$     | 0.001                 | 0.125                  |
| $$\frac{3}{4}$$     | 0.11                  | 0.75                   |
| $$\frac{5}{16}$$    | 0.0101                | 0.3125                 |
| 10$$\frac{11}{16}$$ | 10.1011               | 10.6875                |
| 1$$\frac{1}{8}$$    | 1.001                 | 1.125                  |
| 5$$\frac{7}{8}$$    | 101.111               | 5.875                  |
| 3$$\frac{3}{16}$$   | 11.0011               | 3.1875                 |
{% endtab %}

{% tab title="Solution 2.46" %}
A. $$0.000110011[0011]..._2$$ - $$0.00011001100110011001100_2$$ = $$0.0000000000000000000000011[0011]..._2$$

\
B. 0.1 - x = 0.1 - 0.09999990463256836 = 0.00000009536743164

\
C. 0.00000009536743164 \* 10 \* 60 \* 60 \* 100 = 0.343322753904 (s)

\
D. 0.343322753904 \* 2000 = 686.645507808 (m)
{% endtab %}
{% endtabs %}

## 2.4.2 IEEE Floating-Point Representation

**Positional notation** such as considered in the previous section would not be efficient for representing very large numbers. For example, the representation of $$5 \times 2^{100}$$ would consist of the bit pattern 101 followed by 100 zeros. Instead, we would like to represent numbers in a form $$x \times 2^y$$ by giving the values of x and y.

The IEEE floating-point standard represents a number in a form $$V = (−1)^s \times M \times 2^E$$:

* The **sign s** determines whether the number is negative (s = 1) or positive (s = 0), where the interpretation of the sign bit for numeric value 0 is handled as a special case.
* The **significand M** is a fractional binary number that ranges either between 1 and 2 − $$\epsilon$$ or between 0 and 1 − $$\epsilon$$.
* The **exponent E** weights the value by a (possibly negative) power of 2.

The bit representation of a floating-point number is divided into three fields to encode these values:

* The **single sign bit** s directly encodes the sign s.
* The **k-bit exponent field** $$exp = e_{k−1} ... e_1e_0$$ encodes the exponent E.
* The **n-bit fraction field** $$frac = f_{n−1} ... f_1f_0$$ encodes the significand M, but the value encoded also depends on whether or not the exponent field equals 0.

Figure 2.32 shows the packing of these three fields into words for the two most common formats. In the single-precision floating-point format (a `float` in C), fields s, exp, and frac are **1**, k = **8**, and n = **23** bits each, yielding a 32- bit representation. In the double-precision floating-point format (a double in C), fields s, exp, and frac are **1**, k = **11**, and n = **52** bits each, yielding a 64-bit representa-tion.

![Figure 2.32
Standard floating-point formats.
Floating-point numbers are represented by three fields.
For the two most common formats, these are packed in 32-bit (singleprecision) or 64-bit (double-precision) words.](<../../.gitbook/assets/image (14).png>)

The value encoded by a given bit representation can be divided into three different cases (the latter having two variants), depending on the value of exp. These are illustrated in Figure 2.33 for the single-precision format.

![Figure 2.33
Categories of single-precision floating-point values.
The value of the exponent determines whether the number is (1) normalized, (2) denormalized, or (3) a special value.](<../../.gitbook/assets/image (18).png>)

#### <mark style="color:blue;">Case 1: Normalized Values</mark>

This is the most common case. It occurs when the bit pattern of exp is neither all zeros (numeric value 0) nor all ones (numeric value 255 for single precision, 2047 for double). In this case, the exponent field is interpreted as representing a signed integer in biased form. That is, the exponent value is **E = e − Bias**, where e is the unsigned number having bit representation $$e_{k−1} ... e_1e_0$$ and Bias is a bias value equal to $$2^{k−1} − 1$$ (127 for single precision and 1023 for double). This yields exponent ranges from −126 to +127 for single precision and −1022 to +1023 for double precision.

The fraction field frac is interpreted as representing the fractional value f , where 0 ≤ f < 1, having binary representation $$0.f_{n−1} ... f_1f_0$$, that is, with the binary point to the left of the most significant bit. The significand is defined to be **M = 1 + f**. This is sometimes called an implied leading 1 representation, because we can view M to be the number with binary representation $$1.f_{n−1}f_{n−2} ... f_{0}$$. This representation is a trick for getting an additional bit of precision for free, since we can always adjust the exponent E so that significand M is in the range 1 ≤ M < 2 (assuming there is no overflow). We therefore do not need to explicitly represent the leading bit, since it always equals 1.

#### <mark style="color:blue;">Case 2: Denormalized Values</mark>

When the exponent field is all zeros, the represented number is in denormalized form. In this case, the exponent value is **E = 1 − Bias**, and the significand value is **M = f**, that is, the value of the fraction field without an implied leading 1.

Denormalized numbers serve two purposes.\ <mark style="color:blue;"></mark>First, they provide a way to represent numeric value 0, since with a normalized number we must always have M ≥ 1, and hence we cannot represent 0. In fact, the floating-point representation of +0.0 has a bit pattern of all zeros: the sign bit is 0, the exponent field is all zeros (indicating a denormalized value), and the fraction field is all zeros, giving M = f = 0. Curiously, when the sign bit is 1, but the other fields are all zeros, we get the value −0.0. With IEEE floating-point format, the values −0.0 and +0.0 are considered different in some ways and the same in others.\
A second function of denormalized numbers is to represent numbers that are very close to 0.0. They provide a property known as **gradual underflow** in which possible numeric values are spaced evenly near 0.0.

#### <mark style="color:blue;">Case 3: Special Values</mark>

A final category of values occurs when the exponent field is all ones.\
When the fraction field is all zeros, the resulting values represent **infinity**, either $$+\infty$$ when s = 0 or $$-\infty$$ when s = 1. Infinity can represent results that overflow, as when we multiply two very large numbers, or when we divide by zero. When the fraction field is nonzero, the resulting value is called a **NaN**, short for “not a number.” Such values are returned as the result of an operation where the result cannot be given as a real number or as infinity, as when computing $$\sqrt[]{-1}$$ or $$\infty - \infty$$. They can also be useful in some applications for representing uninitialized data.

{% hint style="info" %}
#### Aside --- Why set the bias this way for denormalized values?

Having the exponent value be 1 − Bias rather than simply −Bias might seem counterintuitive. We will see shortly that it provides for smooth transition from denormalized to normalized values.
{% endhint %}

## 2.4.3 Example Numbers

Figure 2.34 shows the set of values that can be represented in a hypothetical 6-bit format having k = 3 exponent bits and n = 2 fraction bits. The bias is $$2^{3−1} − 1 = 3$$. Part (a) of the figure shows all representable values (other than NaN). The two infinities are at the extreme ends. The normalized numbers with maximum magnitude are ±14. The denormalized numbers are clustered around 0. These can be seen more clearly in part (b) of the figure, where we show just the numbers between −1.0 and +1.0. The two zeros are special cases of denormalized numbers. Observe that the representable numbers are not uniformly distributed — they are denser nearer the origin.

![Figure 2.34
Representable values for 6-bit floating-point format.
There are k = 3 exponent bits and n = 2 fraction bits. The bias is 3.](<../../.gitbook/assets/image (31).png>)

Figure 2.35 shows some examples for a hypothetical 8-bit floating-point format having k = 4 exponent bits and n = 3 fraction bits. The bias is $$2^{4−1} − 1 = 7$$. The figure is divided into three regions representing the three classes of numbers. The different columns show how the exponent field encodes the exponent E, while the fraction field encodes the significand M, and together they form the represented value $$V = 2^E × M$$. Closest to 0 are the denormalized numbers, starting with 0 itself. Denormalized numbers in this format have E = 1 − 7 = −6, giving a weight $$2^E = \frac{1}{64}$$. The fractions f and significands M range over the values 0, $$\frac{1}{8}$$, ..., $$\frac{7}{8}$$, giving numbers V in the range 0 to $$\frac{1}{64} \times \frac{7}{8}$$ = $$\frac{7}{512}$$.

![Figure 2.35
Example nonnegative values for 8-bit floating-point format.
There are k = 4 exponent bits and n = 3 fraction bits. The bias is 7.](<../../.gitbook/assets/image (10).png>)

The smallest normalized numbers in this format also have E = 1 − 7 = −6, and the fractions also range over the values 0, $$\frac{1}{8}$$, ..., $$\frac{7}{8}$$. However, the significands then range from 1 + 0 = 1 to 1 + $$\frac{7}{8}$$ = $$\frac{15}{8}$$, giving numbers V in the range $$\frac{8}{512}$$ = $$\frac{1}{64}$$ to $$\frac{15}{512}$$.

Observe the smooth transition between the largest denormalized number $$\frac{7}{512}$$ and the smallest normalized number $$\frac{8}{512}$$. This smoothness is due to our definition of E for denormalized values. By making it 1 − Bias rather than −Bias, we compensate for the fact that the significand of a denormalized number does not have an implied leading 1.

As we increase the exponent, we get successively larger normalized values, passing through 1.0 and then to the largest normalized number. This number has exponent E = 7, giving a weight $$2^E = 128$$. The fraction equals $$\frac{7}{8}$$, giving a significand M = $$\frac{15}{8}$$. Thus, the numeric value is V = 240. Going beyond this overflows to +∞.

One interesting property of this representation is that if we interpret the bit representations of the values in Figure 2.35 as unsigned integers, they occur in ascending order, as do the values they represent as floating-point numbers. This is no accident — **the IEEE format was designed so that floating-point numbers could be sorted using an integer sorting routine.** A minor difficulty occurs when dealing with negative numbers, since they have a leading 1 and occur in descending order, but this can be overcome without requiring floating-point operations to perform comparisons (see Problem 2.84).

{% tabs %}
{% tab title="Practice Problem 2.47" %}
Consider a 5-bit floating-point representation based on the IEEE floating-point format, with one sign bit, two exponent bits (k = 2), and two fraction bits (n = 2). The exponent bias is $$2^{2−1} − 1 = 1$$.

The table that follows enumerates the entire nonnegative range for this 5-bit floating-point representation. Fill in the blank table entries using the following directions:

e: The value represented by considering the exponent field to be an unsigned integer

E: The value of the exponent after biasing

$$2^E$$: The numeric weight of the exponent

f : The value of the fraction

M: The value of the significand

$$2^E × M$$: The (unreduced) fractional value of the number

V : The reduced fractional value of the number

Decimal: The decimal representation of the number

Express the values of $$2^E$$, f, M, $$2^E × M$$, and V either as integers (when possible) or as fractions of the form $$\frac{x}{y}$$, where y is a power of 2. You need not fill in entries marked —.

![](<../../.gitbook/assets/image (28) (1).png>)

![](<../../.gitbook/assets/image (30).png>)
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Solution 2.47" %}
0 00 00          0       -1          1/2         0       0       0       0         0.0

0 00 01          0       -1          1/2         1/4    1/4    1/8     1/8       0.125

0 00 10          0       -1          1/2        1/2     1/2    1/4     1/4       0.25

0 00 11          0       -1          1/2        3/4    3/4    3/8     3/8       0.375

0 01 00          1       0            1           0       1        1          1           1

0 01 01          1        0           1           1/4     5/4    5/4    5/4         1.25

0 01 10         1       0           1            1/2      3/2    3/2      3/2         1.5

0 01 11          1       0           1            3/4     7/4     7/4     7/4         1.75

0 10 00        2       1          2             0        1         2          2              2

0 10 01        2       1          2             1/4     5/4      5/2      5/2          2.5

0 10 10        2        1         2            1/2      3/2      3         3               3

0 10 11        2        1         2            3/4      7/4     7/2      7/2           3.5
{% endtab %}
{% endtabs %}

Figure 2.36 shows the representations and numeric values of some important single- and double-precision floating-point numbers. As with the 8-bit format shown in Figure 2.35, we can see some general properties for a floating-point representation with a k-bit exponent and an n-bit fraction:

* The value +0.0 always has a bit representation of all zeros.
* The smallest positive denormalized value has a bit representation consisting of a 1 in the least significant bit position and otherwise all zeros. It has a fraction (and significand) value M = f = $$2^{−n}$$ and an exponent value $$E = −2^{k−1} + 2$$. The numeric value is therefore $$V = 2^{−n−2^{k−1}+2}$$.
* The largest denormalized value has a bit representation consisting of an exponent field of all zeros and a fraction field of all ones. It has a fraction (and significand) value $$M = f = 1 − 2^{−n}$$ (which we have written 1 − $$\epsilon$$) and an exponent value $$E = −2^{k−1} + 2$$. The numeric value is therefore $$V = (1 − 2^{−n}) × 2^{−2^{k−1}+2}$$, which is just slightly smaller than the smallest normalized value.
* The smallest positive normalized value has a bit representation with a 1 in the least significant bit of the exponent field and otherwise all zeros. It has a significand value M = 1 and an exponent value $$E = −2^{k−1} + 2$$. The numeric value is therefore $$V = 2^{−2^{k−1}+2}$$.
* The value 1.0 has a bit representation with all but the most significant bit of the exponent field equal to 0 and all other bits equal to 1. Its significand value is M = 1 and its exponent value is E = 0.
* The largest normalized value has a bit representation with a sign bit of 0, the least significant bit of the exponent equal to 0, and all other bits equal to 1. It has a fraction value of $$f = 1 − 2^{−n}$$, giving a significand $$M = 2 − 2^{−n}$$ (which we have written 2 − $$\epsilon$$.) It has an exponent value $$E = 2^{k−1} − 1$$, giving a numeric value $$V = (2 − 2^{−n}) × 2^{2^{k−1}−1} = (1 − 2^{−n−1}) × 2^{2^{k−1}}$$.

![Figure 2.36
Examples of nonnegative floating-point numbers](<../../.gitbook/assets/image (11).png>)

One useful exercise for understanding floating-point representations is to convert sample integer values into floating-point form. For example, we saw in Figure 2.15 that 12,345 has binary representation \[11000000111001]. We create a normalized representation of this by shifting 13 positions to the right of a binary point, giving $$12,345 = 1.1000000111001_2 × 2^{13}$$. To encode this in IEEE single-precision format, we construct the fraction field by dropping the leading 1 and adding 10 zeros to the end, giving binary representation \[10000001110010000000000]. To construct the exponent field, we add bias 127 to 13, giving 140, which has binary representation \[10001100]. We combine this with a sign bit of 0 to get the floating-point representation in binary of \[01000110010000001110010000000000]. Recall from Section 2.1.3 that we observed the following correlation in the bit-level representations of the integer value 12345 (0x3039) and the single-precision floating-point value 12345.0 (0x4640E400):

![](<../../.gitbook/assets/image (28).png>)

We can now see that the region of correlation corresponds to the low-order bits of the integer, stopping just before the most significant bit equal to 1 (this bit forms the implied leading 1), matching the high-order bits in the fraction part of the floating-point representation.

{% tabs %}
{% tab title="Practice Problem 2.48" %}
As mentioned in Problem 2.6, the integer 3,510,593 has hexadecimal representation 0x00359141, while the single-precision floating-point number 3,510,593.0 has hexadecimal representation 0x4A564504. Derive this floating-point representation and explain the correlation between the bits of the integer and floating-point representations.
{% endtab %}

{% tab title="Practice Problem 2.49" %}
A. For a floating-point format with an n-bit fraction, give a formula for the smallest positive integer that cannot be represented exactly (because it would require an (n + 1)-bit fraction to be exact). Assume the exponent field size k is large enough that the range of representable exponents does not provide a limitation for this problem.

B. What is the numeric value of this integer for single-precision format (n = 23)?
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Solution 2.48" %}
3,510,593          0x00359141           0000 0000 001<mark style="color:blue;">1 0101 1001 0001 0100 0001</mark>

3,510,593.0       0x4A564504          0<mark style="color:purple;">100 1010  0</mark><mark style="color:blue;">101  0110 0100 0101 0000 01</mark>00

3,510,593 can be represented by $$1.1 0101 1001 0001 0100 0001_2 \times 2^{21}$$.

so, the M field of 3,510,593.0 is <mark style="color:blue;">101  0110 0100 0101 0000 01</mark>00, adding 2 zeros to the end.

E = 21, Bias = 127, so e = 148,  E field = <mark style="color:purple;">100 1010  0</mark>.
{% endtab %}

{% tab title="Solution 2.49" %}
A. $$2^{n+1} + 1$$

B. $$2^{24} + 1 = 16,777,217$$
{% endtab %}
{% endtabs %}

## 2.4.4 Rounding

Floating-point arithmetic can only approximate real arithmetic, since the representation has limited range and precision. Thus, for a value x, we generally want a systematic method of finding the “closest” matching value x′ that can be represented in the desired floating-point format. This is the task of the rounding operation. **One key problem is to define the direction to round a value that is halfway between two possibilities.** For example, if I have $1.50 and want to round it to the nearest dollar, should the result be $1 or $2? An alternative approach is to maintain a lower and an upper bound on the actual number. For example, we could determine representable values $$x^−$$ and $$x^+$$ such that the value x is guaranteed to lie between them: $$x^−$$ ≤ x ≤ $$x^+$$. **The IEEE floating-point format defines four different rounding modes.** The default method finds the closest match, while the other three can be used for computing upper and lower bounds.

Figure 2.37 illustrates the four rounding modes applied to the problem of rounding a monetary amount to the nearest whole dollar. **Round-to-even** (also called round-to-nearest) is the default mode. It attempts to find the closest match. Thus, it rounds $1.40 to $1 and $1.60 to $2, since these are the closest whole dollar values. The only design decision is to determine the effect of rounding values that are halfway between two possible results. Round-to-even mode adopts the convention that it rounds the number either upward or downward such that the least significant digit of the result is even. Thus, it rounds both $1.50 and $2.50 to $2.

![Figure 2.37
Illustration of rounding modes for dollar rounding.
The first rounds to a nearest value, while the other three bound the result above or below](<../../.gitbook/assets/image (20).png>)

The other three modes produce guaranteed bounds on the actual value. These can be useful in some numerical applications. **Round-toward-zero** mode rounds positive numbers downward and negative numbers upward, giving a value $$\hat{x}$$ such that |$$\hat{x}$$| ≤ |x|. **Round-down** mode rounds both positive and negative numbers downward, giving a value $$x^−$$ such that $$x^−$$ ≤ x. **Round-up** mode rounds both positive and negative numbers upward, giving a value $$x^+$$ such that x ≤ $$x^+$$.

Round-to-even at first seems like it has a rather arbitrary goal—why is there any reason to prefer even numbers? Why not consistently round values halfway between two representable values upward? The problem with such a convention is that one can easily imagine scenarios in which rounding a set of data values would then introduce a statistical bias into the computation of an average of the values. The average of a set of numbers that we rounded by this means would be slightly higher than the average of the numbers themselves. Conversely, if we always rounded numbers halfway between downward, the average of a set of rounded numbers would be slightly lower than the average of the numbers themselves. Rounding toward even numbers avoids this statistical bias in most real-life situations. It will round upward about 50% of the time and round downward about 50% of the time.

Round-to-even rounding can be applied even when we are not rounding to a whole number. We simply consider whether the least significant digit is even or odd. For example, suppose we want to round decimal numbers to the nearest hundredth. We would round 1.2349999 to 1.23 and 1.2350001 to 1.24, regardless of rounding mode, since they are not halfway between 1.23 and 1.24. On the other hand, we would round both 1.2350000 and 1.2450000 to 1.24, since 4 is even.

Similarly, round-to-even rounding can be applied to binary fractional numbers. We consider the least significant bit value 0 to be even and 1 to be odd. In general, the rounding mode is only significant when we have a bit pattern of form $$XX \cdot\cdot\cdot X.YY \cdot\cdot\cdot Y100 \cdot\cdot\cdot$$, where X and Y denote arbitrary bit values with the rightmost Y being the position to which we wish to round. Only bit patterns of this form denote values that are halfway between two possible results. For example, consider the problem of rounding values to the nearest quarter (i.e., 2 bits to the right of the binary point.) We would round $$10.00011_2$$ (2$$\frac{3}{32}$$) down to $$10.00_2$$ (2), and $$10.00110_2$$ (2$$\frac{3}{16}$$) up to $$10.01_2$$ (2$$\frac{1}{4}$$), because these values are not halfway between two possible values. We would round $$10.11100_2$$ ($$2 \frac{7}{8}$$) up to $$11.00_2$$ (3) and $$10.10100_2$$ ($$2\frac{5}{8}$$) down to $$10.10_2$$ ($$2\frac{1}{2}$$), since these values are halfway between two possible results, and we prefer to have the least significant bit equal to zero.

{% tabs %}
{% tab title="PP 2.50" %}
Show how the following binary fractional values would be rounded to the nearest half (1 bit to the right of the binary point), according to the round-to-even rule. In each case, show the numeric values, both before and after rounding.

A. $$10.111_2$$

B. $$11.010_2$$

C. $$11.000_2$$

D. $$10.110_2$$
{% endtab %}

{% tab title="PP 2.51" %}
We saw in Problem 2.46 that the Patriot missile software approximated 0.1 as $$x = 0.00011001100110011001100_2$$. Suppose instead that they had used IEEE round-to-even mode to determine an approximation x′ to 0.1 with 23 bits to the right of the binary point.

A. What is the binary representation of x′?

B. What is the approximate decimal value of x′ − 0.1?

C. How far off would the computed clock have been after 100 hours of operation?

D. How far off would the program’s prediction of the position of the Scud missile have been?
{% endtab %}

{% tab title="PP 2.52" %}
Consider the following two 7-bit floating-point representations based on the IEEE floating-point format. Neither has a sign bit — they can only represent nonnegative numbers.

1. Format A
   1. There are k = 3 exponent bits. The exponent bias is 3.
   2. There are n = 4 fraction bits.
2. Format B
   1. There are k = 4 exponent bits. The exponent bias is 7.
   2. There are n = 3 fraction bits.

Below, you are given some bit patterns in format A, and your task is to convert them to the closest value in format B. If necessary, you should apply the round-to-even rounding rule. In addition, give the values of numbers given by the format A and format B bit patterns. Give these as whole numbers (e.g., 17) or as fractions (e.g., 17/64).

![](<../../.gitbook/assets/image (29).png>)
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="S 2.50" %}
A. $$10.111_2$$      $$11.0_2$$        2$$\frac{7}{8}$$      3

B. $$11.010_2$$      $$11.0_2$$        3$$\frac{1}{4}$$      3

C. $$11.000_2$$      $$11.0_2$$        3         3

D. $$10.110_2$$      $$11.0_2$$       2$$\frac{3}{4}$$       3
{% endtab %}

{% tab title="S 2.51" %}
A. x' = <mark style="color:blue;"></mark> $$0.0001100110 0110011001101_2$$

B. x' - 0.1 = 0.10000002384185791 - 0.1 = 0.00000002384185791 (s)

C. 0.00000002384185791 \* 100 \* 60 \* 60 \* 10 = 0.085830688476 (s)

D. 0.085830688476 \* 2000 = 171.661376952 (m)
{% endtab %}

{% tab title="S 2.52" %}
101 1110           15/2           1011 111          15/2

010 1001          25/32        0110 100        3/4

110 1111           31/2           1011 000        16

000 0001        1/64           0001 000       1/64
{% endtab %}
{% endtabs %}

## 2.4.5 Floating-Point Operations

## 2.4.6 Floating Point in C
