# 2.4 Floating Point

A floating-point representation encodes rational numbers of the form V = $$x × 2^y$$. It is useful for performing computations involving very large numbers (|V| $$\gg$$ 0), numbers very close to 0 (|V| $$\ll$$ 1), and more generally as an approximation to real arithmetic.

Up until the 1980s, every computer manufacturer devised its own conventions for how floating-point numbers were represented and the details of the operations performed on them. In addition, they often did not worry too much about the accuracy of the operations, viewing speed and ease of implementation as being more critical than numerical precision.

All of this changed around 1985 with the advent of **IEEE Standard 754**, a carefully crafted standard for representing floating-point numbers and the operations performed on them. This effort started in 1976 under Intel’s sponsorship with the design of the 8087, a chip that provided floating-point support for the 8086 processor. Intel hired William Kahan, a professor at the University of California, Berkeley, as a consultant to help design a floating-point standard for its future processors. They allowed Kahan to join forces with a committee generating an industry-wide standard under the auspices of the Institute of Electrical and Electronics Engineers (IEEE). The committee ultimately adopted a standard close to the one Kahan had devised for Intel. Nowadays, virtually all computers support what has become known as IEEE floating point. This has greatly improved the portability of scientific application programs across different machines.

In this section, we will see how numbers are represented in the IEEE floatingpoint format. We will also explore issues of **rounding**, when a number cannot be represented exactly in the format and hence must be adjusted upward or downward. We will then explore the mathematical properties of addition, multiplication, and relational operators. Many programmers consider floating point to be at best uninteresting and at worst arcane and incomprehensible. We will see that since the IEEE format is based on a small and consistent set of principles, it is really quite elegant and understandable.

{% hint style="info" %}
#### Aside --- The IEEE

The Institute of Electrical and Electronics Engineers (IEEE — pronounced “eye-triple-ee”) is a professional society that encompasses all of electronic and computer technology. It publishes journals, sponsors conferences, and sets up committees to define standards on topics ranging from power transmission to software engineering. Another example of an IEEE standard is the 802.11 standard for wireless networking.
{% endhint %}

## 2.4.1 Fractional Binary Numbers

A first step in understanding floating-point numbers is to consider binary numbers having fractional values. Let us first examine the more familiar decimal notation. Decimal notation uses a representation of the form

$$
d_m d_{m−1} ... d_1 d_0 . d_{−1} d_{−2} ... d_{−n}
$$

where each decimal digit $$d_i$$ ranges between 0 and 9. This notation represents a value d defined as

$$
d = \sum ^m_{i=-n} 10^i \times d_i
$$

The weighting of the digits is defined relative to the **decimal point** symbol (‘.’), meaning that digits to the left are weighted by nonnegative powers of 10, giving integral values, while digits to the right are weighted by negative powers of 10, giving fractional values. For example, 12.3410 represents the number $$1 \times 10^1 + 2 \times 10^0 + 3 \times 10^{−1} + 4 \times 10^{−2} = 12 \frac{34}{100}$$.

By analogy, consider a notation of the form

$$
b_m b_{m−1} ... b_1 b_0 . b_{−1} b_{−2} ... b_{−n}
$$

where each binary digit, or bit, $$b_i$$ ranges between 0 and 1, as is illustrated in Figure 2.31. This notation represents a number b defined as

$$
b = \sum^m_{i=-n} 2^i \times b_i \tag{2.19}
$$

![Figure 2.31
Fractional binary representation.
Digits to the left of the binary point have weights of the form 2^i , while those to the right have weights of the form 1/2^i .](<../../.gitbook/assets/image (32).png>)

The symbol ‘.’ now becomes a binary point, with bits on the left being weighted by nonnegative powers of 2, and those on the right being weighted by negative powers of 2. For example, $$101.11_2$$ represents the number $$1 × 2^2 + 0 × 2^1 + 1 × 2^0 + 1 × 2^{−1} + 1 × 2^{−2} = 4 + 0 + 1 + \frac{1}{2} + \frac{1}{4} = 5\frac{3}{4}$$.

One can readily see from Equation 2.19 that shifting the binary point one position to the left has the effect of dividing the number by 2. For example, while 101.112 represents the number $$5\frac{3}{4}$$, $$10.111_2$$ represents the number 2 + 0 + 1/2 + 1/4 + 1/8 = 2$$\frac{7}{8}$$. Similarly, shifting the binary point one position to the right has the effect of multiplying the number by 2. For example, $$1011.1_2$$ represents the number 8 + 0 + 2 + 1 + 1/2 = 11$$\frac{1}{2}$$.

Note that numbers of the form $$0.11 ... 1_2$$ represent numbers just below 1. For example, $$0.111111_2$$ represents $$\frac{63}{64}$$. We will use the shorthand notation $$1.0 − \epsilon$$ to represent such values.

Assuming we consider only finite-length encodings, decimal notation cannot represent numbers such as 1/3 and 5/7 exactly. Similarly, fractional binary notation can only represent numbers that can be written $$x \times 2^y$$. Other values can only be approximated. For example, the number 1/5 can be represented exactly as the fractional decimal number 0.20. As a fractional binary number, however, we cannot represent it exactly and instead must approximate it with increasing accuracy by lengthening the binary representation:

![](<../../.gitbook/assets/image (9).png>)

{% tabs %}
{% tab title="Practice Problem 2.45" %}
Fill in the missing information in the following table:

| Fractional value | Binary representation | Decimal representation |
| ---------------- | --------------------- | ---------------------- |
| $$\frac{1}{8}$$  | 0.001                 | 0.125                  |
| $$\frac{3}{4}$$  |                       |                        |
| $$\frac{5}{16}$$ |                       |                        |
|                  | 10.1011               |                        |
|                  | 1.001                 |                        |
|                  |                       | 5.875                  |
|                  |                       | 3.1875                 |
{% endtab %}

{% tab title="Practice Problem 2.46" %}
The imprecision of floating-point arithmetic can have disastrous effects. On February 25, 1991, during the first Gulf War, an American Patriot Missile battery in Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud missile. The Scud struck an American Army barracks and killed 28 soldiers. The US General Accounting Office (GAO) conducted a detailed analysis of the failure \[76] and determined that the underlying cause was an imprecision in a numeric calculation. In this exercise, you will reproduce part of the GAO’s analysis.

The Patriot system contains an internal clock, implemented as a counter that is incremented every 0.1 seconds. To determine the time in seconds, the program would multiply the value of this counter by a 24-bit quantity that was a fractional binary approximation to 1 10 . In particular, the binary representation of 1 10 is the nonterminating sequence 0.000110011\[0011]...2, where the portion in brackets is repeated indefinitely. The program approximated 0.1, as a value x, by considering just the first 23 bits of the sequence to the right of the binary point: x = 0.00011001100110011001100. (See Problem 2.51 for a discussion of how they could have approximated 0.1 more precisely.)

A. What is the binary representation of 0.1 − x?

B. What is the approximate decimal value of 0.1 − x?

C. The clock starts at 0 when the system is first powered up and keeps counting up from there. In this case, the system had been running for around 100 hours. What was the difference between the actual time and the time computed by the software?

D. The system predicts where an incoming missile will appear based on its velocity and the time of the last radar detection. Given that a Scud travels at around 2,000 meters per second, how far off was its prediction?

Normally, a slight error in the absolute time reported by a clock reading would not affect a tracking computation. Instead, it should depend on the relative time between two successive readings. The problem was that the Patriot software had been upgraded to use a more accurate function for reading time, but not all of the function calls had been replaced by the new code. As a result, the tracking software used the accurate time for one reading and the inaccurate time for the other \[103].
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Solution 2.45" %}


| Fractional value | Binary representation | Decimal representation |
| ---------------- | --------------------- | ---------------------- |
| $$\frac{1}{8}$$  | 0.001                 | 0.125                  |
| $$\frac{3}{4}$$  |                       |                        |
| $$\frac{5}{16}$$ |                       |                        |
|                  | 10.1011               |                        |
|                  | 1.001                 |                        |
|                  |                       | 5.875                  |
|                  |                       | 3.1875                 |
{% endtab %}

{% tab title="Solution 2.46" %}

{% endtab %}
{% endtabs %}

## 2.4.2 IEEE Floating-Point Representation

## 2.4.3 Example Numbers

## 2.4.4 Rounding

## 2.4.5 Floating-Point Operations

## 2.4.6 Floating Point in C
