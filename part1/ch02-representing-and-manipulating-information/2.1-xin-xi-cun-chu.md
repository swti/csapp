# 2.1 信息存储

大多数计算机使用 8 位的块，或者**字节**（byte），作为最小的可寻址的内存单位，而不是访问内存中单独的位。机器级程序将内存视为一个非常大的字节数组，称为**虚拟内存**（virtual memory）。内存的每个字节都由一个唯一的数字来标识，称为它的**地址**（address），所有可能地址的集合就称为**虚拟地址空间**（virtual address space）。顾名思义，这个虚拟地址空间只是一个展现给机器级程序的概念性映像。实际的实现（见第 9 章）是将动态随机访问存储器（DRAM）、闪存、磁盘存储器、特殊硬件和操作系统软件结合起来，为程序提供一个看上去统一的字节数组。

在接下来的几章中，我们将讲述编译器和运行时系统是如何将存储器空间划分为更可管理的单元，来存放不同的程序对象（program object），即程序数据、指令和控制信息。可以用各种机制来分配和管理程序不同部分的存储。这种管理完全是在虚拟地址空间里完成的。例如，C 语言中一个指针的值（无论它指向一个整数、一个结构或是某个其他程序对象）都是某个存储块的第一个字节的虚拟地址。C 编译器还把每个指针和类型信息联系起来，这样就可以根据指针值的类型，生成不同的机器级代码来访问存储在指针所指向位置处的值。尽管 C 编译器维护着这个类型信息，但是它生成的实际机器级程序并不包含关于数据类型的信息。每个程序对象可以简单地视为一个字节块，而程序本身就是一个字节序列。

{% hint style="info" %}
#### 给 C 语言初学者 - C 语言中指针的作用

指针是 C 语言的一个重要特性。它提供了引用数据结构（包括数组）的元素的机制。与变量类似，指针也有两个方面：值和类型。它的值表示某个对象的位置，而它的类型表示那个位置上所存储对象的类型（比如整数或者浮点数）。

真正理解指针需要查看它们在机器级上的表示以及实现。这将是第 3 章的重点之 一，3.10.1 节将对其进行深入介绍。
{% endhint %}

## 2.1.1 十六进制表示法

一个字节由8位组成。在二进制表示法中，它的值域是 $$00000000_2$$\~$$11111111_2$$。如果看成十进制整数，它的值域就是$$0_{10}$$\~$$255_{10}$$。两种符号表示法对于描述位模式来说都不是非常方便。二进制表示法太冗长，而十进制表示法与位模式的互相转化很麻烦。替代的方法是，以 16 为基数，或者叫做_十六进制（hexadecimal）数_，来表示位模式。十六进制（简写为 "hex"） 使用数字 '0' \~ '9' 以及字符 'A' \~ 'F' 来表示 16 个可能的值。图 2-2 展示了 16 个十六进制数字对应的十进制值和二进制值。用十六进制书写，一个字节的值域为$$00_{16}$$\~$$FF_{16}$$。

![Figure 2.2 Hexadecimal notation. Each hex digit encodes one of 16 values.](<../../.gitbook/assets/image (1) (1).png>)

在 C 语言中，以 0x 或 0X 开头的数字常量被认为是十六进制的值。字符 ‘A' \~ ‘F’ 既可以是大写，也可以是小写。例如，我们可以将数字 FA1D37B16 写作 0xFA1D37B，或者 0xfald37b，甚至是大小写混合，比如，0XFa1D37b。在本书中，我们将使用 C 表示法来表示十六进制值。&#x20;

编写机器级程序的一个常见任务就是在位模式的十进制、二进制和十六进制表示之间人工转换。二进制和十六进制之间的转换比较简单直接，因为可以一次执行一个十六进制数字的转换。数字的转换可以参考如图 2-2 所示的表。<mark style="color:green;">一个简单的窍门是，记住十六进制数字 A、C 和 F 相应的十进制值。而对于把十六进制值 B、D 和 E 转换成十进制值，则可以通过计算它们与前三个值的相对关系来完成。</mark>

比如，假设给你一个数字 0x173A4C。可以通过展开每个十六进制数字，将它转换为二进制格式，如下所示：

![0x173A4C to binary](<../../.gitbook/assets/image (2).png>)

这样就得到了二进制表示 000101110011101001001100。&#x20;

反过来，如果给定一个二进制数字1111001010110110110011，可以通过首先把它分为每 4 位一组来转换为十六进制。不过要注意，如果位总数不是 4 的倍数，最左边的一组可以少于 4 位，前面用 0 补足。然后将每个 4 位组转换为相应的十六进制数字：

![1111001010110110110011 to hexadecimal](<../../.gitbook/assets/image (2) (1).png>)

当值 x 是 2 的非负整数 n 次幂时，也就是 $$x=2^n$$，我们可以很容易地将 x 写成十六进制形式，只要记住x 的二进制表示就是 1 后面跟 n 个 0。十六进制数字 0 代表 4 个二进制 0。所以，当 n 表示成 i+4j 的形式，其中 0≤i≤3，我们可以把 x 写成开头的十六进制数字为 1(i=0)、2(i=1)、4(i=2) 或者8(i=3)，后面跟随着 i 个十六进制的 0。比如, $$x=2048=2^{11}$$，我们有 n=11=3+4×2，从而得到十六进制表示 0x800。

{% tabs %}
{% tab title="Practice Problem 2.1" %}
Perform the following number conversions:&#x20;



A. 0x25B9D2 to binary&#x20;

B. binary 1010111001001001 to hexadecimal&#x20;

C. 0xA8B3D to binary&#x20;

D. binary 1100100010110110010110 to hexadecimal
{% endtab %}

{% tab title="Practice Problem 2.2" %}
Fill in the blank entries in the following table, giving the decimal and hexadecimal representations of different powers of 2:

|  n  | 2^n (十进制) | 2^n (十六进制) |
| :-: | :-------: | :--------: |
|  9  |    512    |    0x200   |
|  19 |           |            |
|     |   16384   |            |
|     |           |   0x10000  |
|  17 |           |            |
|     |     32    |            |
|     |           |    0x80    |
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Solution 2.1" %}
A. 0010 1001 1011 1001 1101 0010

B. AE69

C. 1010 1000 1011 0011 1101

D. 322D96
{% endtab %}

{% tab title="Solution 2.2" %}
|  n  | 2^n (十进制) | 2^n (十六进制) |
| :-: | :-------: | :--------: |
|  9  |    512    |    0x200   |
|  19 |   524288  |   0x80000  |
|  14 |   16384   |   0x4000   |
|  16 |   65536   |   0x10000  |
|  17 |   131072  |   0x20000  |
|  5  |     32    |    0x20    |
|  7  |    128    |    0x80    |
{% endtab %}
{% endtabs %}

十进制和十六进制表示之间的转换需要使用乘法或者除法来处理一般情况。将一个十进制数字 x 转换为十六进制，可以反复地用 16 除 x，得到一个商 q 和一个余数 r，也就是 x=q·16+r。然后，我们用十六进制数字表示的 r 作为最低位数字，并且通过对 q 反复进行这个过程得到剩下的数字。例如，考虑十进制 314156的转换：

![](<../../.gitbook/assets/image (4) (1).png>)

从这里，我们能读出十六进制表示为 0x4CB2C。&#x20;

反过来，将一个十六进制数字转换为十进制数字，我们可以用相应的 16 的幂乘以每个十六进制数字。比如，给定数字 0x7AF，我们计算它对应的十进制值为 7·16^2+10· 16+15=7·256+10·16+15=1792+160+15=1967。

{% tabs %}
{% tab title="Practice Problem 2.3" %}
A single byte can be represented by 2 hexadecimal digits. \
Fill in the missing entries in the following table, giving the decimal, binary, and hexadecimal values of different byte patterns:

| 十进制 |    二进制    | 十六进制 |
| :-: | :-------: | :--: |
|  0  | 0000 0000 | 0x00 |
| 167 |           |      |
|  62 |           |      |
| 188 |           |      |
|     | 0011 0111 |      |
|     | 1000 1000 |      |
|     | 1111 0011 |      |
|     |           | 0x52 |
|     |           | 0xAC |
|     |           | 0xE7 |
{% endtab %}

{% tab title="Practice Problem 2.4" %}
Without converting the numbers to decimal or binary, try to solve the following arithmetic problems, giving the answers in hexadecimal. \
_Hint: Just modify the methods you use for performing decimal addition and subtraction to use base 16._

A. 0x605c + 0x5 =&#x20;

B. 0x605c − 0x20 =&#x20;

C. 0x605c + 32 =&#x20;

D. 0x60fa − 0x605c =
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Solution 2.3" %}
| 十进制 |    二进制    | 十六进制 |
| :-: | :-------: | :--: |
|  0  | 0000 0000 | 0x00 |
| 167 | 1010 0111 | 0xA7 |
|  62 | 0011 1110 | 0x3E |
| 188 | 1011 1100 | 0xBC |
|  55 | 0011 0111 | 0x37 |
| 136 | 1000 1000 | 0x88 |
| 243 | 1111 0011 | 0xF3 |
|  82 | 0101 0010 | 0x52 |
| 172 | 1010 1100 | 0xAC |
| 231 | 1110 0111 | 0xE7 |
{% endtab %}

{% tab title="Solution 2.4" %}
A. 0x605c + 0x5 = 0x6061

B. 0x605c − 0x20 = 0x603c

C. 0x605c + 32 = 0x607c

D. 0x60fa − 0x605c =0x9e
{% endtab %}
{% endtabs %}

{% hint style="info" %}
#### 十进制和十六进制间的转换&#x20;

较大数值的十进制和十六进制之间的转换，最好是让计算机或者计算器来完成。有大量的工具可以完成这个工作。\
一个简单的方法就是利用任何标准的搜索引擎，比如查询：&#x20;

Convert 0xabcd to decimal

or

123 in hex
{% endhint %}

## 2.1.2 字数据大小

每台计算机都有一个字长（word size），指明指针数据的标称大小（nominal size）。因为虚拟地址是以这样的一个字来编码的，所以字长决定的最重要的系统参数就是虚拟地址空间的最大大小。也就是说，对于一个字长为 w 位的机器而言，虚拟地址的范围为 0\~$$2^w$$-1，程序最多访问个 $$2^w$$ 字节。&#x20;

最近这些年，出现了大规模的从 32 位字长机器到 64 位字长机器的迁移。这种情况首先出现在为大型科学和数据库应用设计的高端机器上，之后是台式机和笔记本电脑，最近则出现在智能手机的处理器上。32 位字长限制虚拟地址空间为 4 千兆字节（写作 4GB），也就是说，刚刚超过 4×$$10^9$$ 字节。扩展到 64 位字长使得虚拟地址空间为 16EB，大约是 1.84×$$10^{19}$$ 字节。

> 1EB = 1024PB
>
> 1PB=1024TB
>
> 1TB = 1024GB
>
> 1GB = 1024MB
>
> 1MB = 1024KB
>
> 1KB = 1024B

大多数 64 位机器也可以运行为 32 位机器编译的程序，这是一种向后兼容。因此，举例来说，当程序`prog.c` 用如下伪指令编译后:

```shell
gcc -m32 prog.c
```

该程序就可以在 32 位或 64 位机器上正确运行。另一方面，若程序用下述伪指令编译:

```
gcc -m64 prog.c
```

那就只能在 64 位机器上运行。<mark style="color:green;">因此，我们将程序称为 “32 位程序” 或 “64 位程序” 时，区别在于该程序是如何编译的，而不是其运行的机器类型。</mark>&#x20;

计算机和编译器支持多种不同方式编码的数字格式，如不同长度的整数和浮点数。比如，许多机器都有处理单个字节的指令，也有处理表示为 2 字节、4 字节或者 8 字节整数的指令，还有些指令支持表示为 4 字节和 8 字节的浮点数。

C 语言支持整数和浮点数的多种数据格式。图2-3展示了为 C 语言各种数据类型分配的字节数。（我们将在 2.2 节讨论 C 标准保证的字节数和典型的字节数之间的关系。）有些数据类型的确切字节数依赖于程序是如何被编译的。我们给出的是 32 位和 64 位程序的典型值。整数或者为有符号的，即可以表示负数、零和正数；或者为无符号的，即只能表示非负数。C 的数据类型 char 表示一个单独的字节。尽管 “char” 是由于它被用来存储文本串中的单个字符这一事实而得名，但它也能被用来存储整数值。数据类型 short、int 和 long 可以提供各种数据大小。即使是为 64 位系统编译，数据类型 int 通常也只有 4 个字节。数据类型 long 一般在 32 位程序中为 4 字节，在 64 位程序中则为 8 字节。

![Figure 2.3 Typical sizes (in bytes) of basic C data types. The number of bytes allocated varies with how the program is compiled. This chart shows the values typical of 32-bit and 64-bit programs.](../../.gitbook/assets/image.png)

为了避免由于依赖“典型”大小和不同编译器设置带来的奇怪行为，ISO C99 引入了一类数据类型，其数据大小是固定的，不随编译器和机器设置而变化。其中就有数据类型 `int32t` 和 `int64t`，它们分别为 4 个字节和 8 个字节。使用确定大小的整数类型是程序员准确控制数据表示的最佳途径。&#x20;

大部分数据类型都编码为有符号数值，除非有前缀关键字 unsigned 或对确定大小的数据类型使用了特定的无符号声明。<mark style="color:green;">数据类型 char 是一个例外。尽管大多数编译器和机器将它们视为有符号数，但 C 标准不保证这一点。相反，正如方括号指示的那样，程序员应该用有符号字符的声明来保证其为一个字节的有符号数值。</mark>不过，在很多情况下，程序行为对数据类型 char 是有符号的还是无符号的并不敏感。&#x20;

对关键字的顺序以及包括还是省略可选关键字来说，C 语言允许存在多种形式。比如，下面所有的声明都是一个意思：

unsigned long&#x20;

unsigned long int&#x20;

long unsigned&#x20;

long unsigned int

我们将始终使用图 2-3 给出的格式。 图 2-3 还展示了指针（例如一个被声明为类型为 `char*` 的变量）使用程序的全字长。大多数机器还支持两种不同的浮点数格式：单精度（在  C 中声明为 `float`）和双精度（在 C 中声明为 `double`）。这些格式分别使用 4 字节和 8 字节。

{% hint style="info" %}
#### 给C语言初学者 --- 声明指针

对于任何数据类型 T，声明&#x20;

T \*p;

表明 p 是一个指针变量，指向一个类型为 T 的对象。例如:

char \*p;

就将一个指针声明为指向一个 char 类型的对象。
{% endhint %}

程序员应该力图使他们的程序在不同的机器和编译器上可移植。可移植性的一个方面就是使程序对不同数据类型的确切大小不敏感。C 语言标准对不同数据类型的数字范围设置了下界（这点在后面还将讲到），但是却没有上界。因为从 1980 年左右到 2010 年左右，3 2位机器和 32 位程序是主流的组合，许多程序的编写都假设为图 2-3 中 32 位程序的字节分配。随着 64 位机器的日益普及，在将这些程序移植到新机器上时，许多隐藏的对字长的依赖性就会显现出来，成为错误。<mark style="color:green;">比如，许多程序员假设一个声明为</mark> <mark style="color:green;"></mark><mark style="color:green;">`int`</mark> <mark style="color:green;"></mark><mark style="color:green;">类型的程序对象能被用来存储一个指针。这在大多数 32 位的机器上能正常工作，但是在一台 64 位的机器上却会导致问题。</mark>

## 2.1.3 寻址和字节顺序

对于跨越多字节的程序对象，我们必须建立两个规则：这个对象的地址是什么，以及在内存中如何排列这些字节。在几乎所有的机器上，多字节对象都被存储为连续的字节序列，对象的地址为所使用字节中最小的地址。例如，假设一个类型为 `int` 的变量 x 的地址为 0x100，也就是说，地址表达式 `&x` 的值为0x100。那么，（假设数据类型 int 为 32 位表示）x 的 4 个字节将被存储在内存的 0x100、0x101、0×102 和 0×103 位置。

排列表示一个对象的字节有两个通用的规则。考虑一个 w 位的整数，其位表示为\
\[$$x_{w-1}$$，$$x_{w-2}$$，…，$$x_1$$，$$x_0$$]，其中 $$x_{w-1}$$ 是最高有效位，而 $$x_0$$ 是最低有效位。假设 w 是 8 的倍数，这些位就能被分组成为字节，其中最高有效字节包含位\[$$x_{w-1}$$，$$x_{w-2}$$，…，$$x_{w-8}$$]，而最低有效字节包含位\[$$x_7$$，$$x_6$$，…，$$x_0$$]，其他字节包含中间的位。某些机器选择在内存中按照从最低有效字节到最高有效字节的顺序存储对象，而另一些机器则按照从最高有效字节到最低有效字节的顺序存储。前一种规则——最低有效字节在最前面的方式，称为**小端法（little endian）**。 后一种规则——最高有效字节在最前面的方式，称为**大端法（big endian）**。

假设变量 x 的类型为 `int`，位于地址 0x100 处，它的十六进制值为0x01234567。地址范围 0x100\~0x103 的字节顺序依赖于机器的类型：

![](<../../.gitbook/assets/image (1).png>)

注意，在字 0x01234567 中，高位字节的十六进制值为 0x01，而低位字节值为 0×67。

大多数 Intel 兼容机都只用小端模式。另一方面，IBM 和 Oracle (从其 2010 年收购 Sun Microsystems 开始) 的大多数机器则是按大端模式操作。注意我们说的是“大多数”。 这些规则并没有严格按照企业界限来划分。比如，IBM 和 Oracle 制造的个人计算机使用的是 Intel 兼容的处理器，因此使用小端法。许多比较新的微处理器是**双端法（bi-endian）**，也就是说可以把它们配置成作为大端或者小端的机器运行。然而，实际情况是：一旦选择了特定操作系统，那么字节顺序也就固定下来。比如，用于许多移动电话的 ARM 微处理器，其硬件可以按小端或大端两种模式操作，但是这些芯片上最常见的两种操作系统 — Android（来自Google）和 IOS（来自 Apple）——却只能运行于小端模式。&#x20;

令人吃惊的是，在哪种字节顺序是合适的这个问题上，人们表现得非常情绪化。实际上，术语 “little endian (小端)”和 “big endian (大端)” 出自 Jonathan Swift 的《格利佛游记》(Gulliver's Travels) 一书，其中交战的两个派别无法就应该从哪一端（小端还是大端）打开一个半熟的鸡蛋达成一致。就像鸡蛋的问题一样，选择何种字节顺序没有技术上的理由，因此争论沦为关于社会政治论题的争论。只要选择了一种规则并且始终如一地坚持，对于哪种字节排序的选择都是任意的。

{% hint style="info" %}
#### 旁注 --- Origin of “endian”

以下是 Jonathan Swift 在 1726 年关于大小端之争历史的描述：

".……我下面要告诉你的是，Lilliput 和 Blefuscu 这两大强国在过去 36 个月里一直在苦战。战争开始是由于以下的原因：我们大家都认为，吃鸡蛋前，原始的方法是打破鸡蛋较大的一端，可是当今皇帝的祖父小时候吃鸡蛋，一次按古法打鸡蛋时碰巧将一个手指弄破了，因此他的父亲，当时的皇帝，就下了一道救令，命令全体臣民吃鸡蛋时打破鸡蛋较小的一端，违令者重罚。老百姓们对这项命令极为反感。历史告诉我们，由此曾发生过六次叛乱，其中一个皇帝送了命，另一个丢了王位。这些叛乱大多都是由 Ble- fuscu 的国王大臣们煽动起来的。叛乱平息后，流亡的人总是逃到那个帝国去寻救避难。据估计，先后几次有 11000 人情愿受死也不肯去打破鸡蛋较小的一端。关于这一争端，曾出版过几百本大部著作，不过大端派的书一直是受禁的，法律也规定该派的任何人不得做官。"（此段译文摘自网上蒋剑锋译的《格利佛游记》第一卷第 4 章。）

在他那个时代，Swift 是在讽刺英国（Lilliput）和法国（Blefuscu）之间持续的冲突。 Danny Cohen，一位网络协议的早期开创者，第一次使用这两个术语来指代字节顺序 \[24]，后来这个术语被广泛接纳了。
{% endhint %}

对于大多数应用程序员来说，其机器所使用的字节顺序是完全不可见的。无论为哪种类型的机器所编译的程序都会得到同样的结果。<mark style="color:green;">不过有时候，字节顺序会成为问题。首先是在不同类型的机器之间通过网络传送二进制数据时</mark>，一个常见的问题是当小端法机器产生的数据被发送到大端法机器或者反过来时，接收程序会发现，字里的字节成了反序的。为了避免这类问题，网络应用程序的代码编写必须遵守已建立的关于字节顺序的规则，以确保发送方机器将它的内部表示转换成网络标准，而接收方机器则将网络标准转换为它的内部表示。我们将在第11章中看到这种转换的例子。&#x20;

<mark style="color:green;">第二种情况是，当阅读表示整数数据的字节序列时字节顺序也很重要。这通常发生在检查机器级程序时。</mark>作为一个示例，从某个文件中摘出了下面这行代码，该文件给出了一个针对 Intel ×86-64 处理器的机器级代码的文本表示：

```
4004d3: 01 05 43 0b 20 00         add     %eax, 0x200b43(%rip)
```

这一行是由**反汇编器（disassembler）**生成的，反汇编器是一种确定可执行程序文件所表示的指令序列的工具。我们将在第 3 章中学习有关这些工具的更多知识，以及怎样解释像这样的行。而现在，我们只是注意这行表述的意思是：十六进制字符串 01 05 43 0b 20 00 是一条指令的字节级表示，这条指令是把一个字长的数据加到一个值上，该值的存储地址由 0x200b43 加上当前程序计数器的值得到，当前程序计数器的值即为下一条将要执行指令的地址。如果取出这个序列的最后 4 个字节：43 0b 20 00，并且按照相反的顺序写出，我们得到 00 20 0b 43。去掉开头的 0，得到值 0x200b43，这就是右边的数值。当阅读像此类小端法机器生成的机器级程序表示时，经常会将字节按照相反的顺序显示。书写字节序列的自然方式是最低位字节在左边，而最高位字节在右边，这正好和通常书写数字时最高有效位在左边，最低有效位在右边的方式相反。&#x20;

字节顺序变得重要的第三种情况是当编写规避正常的类型系统的程序时。在 C 语言中，可以通过使用**强制类型转换（cast）**或**联合（union）**来允许以一种数据类型引用一个对象，而这种数据类型与创建这个对象时定义的数据类型不同。大多数应用编程都强烈不推荐这种编码技巧，但是它们对系统级编程来说是非常有用，甚至是必需的。&#x20;

下面展示了一段 C 代码，它使用强制类型转换来访问和打印不同程序对象的字节表示。我们用 `typedef` 将数据类型 `byte_pointer` 定义为一个指向类型为 `unsigned char` 的对象的指针。这样一个字节指针引用一个字节序列，其中每个字节都被认为是一个非负整数。第一个例程 `show_bytes` 的输入是一个字节序列的地址，它用一个字节指针以及一个字节数来指示。该字节数指定为数据类型 `size_t`，表示数据结构大小的首选数据类型。`show_bytes` 打印出每个以十六进制表示的字节。C 格式化指令 `%.2x` 表明整数必须用至少两个数字的十六进制格式输出。

```c
/// 打印程序对象的字节表示。
/// 这段代码使用强制类型转换来规避类型系统。 很容易定义针对其他数据类型的类似函数

#include <stdio.h>

typedef unsigned char* byte_pointer;

void show_bytes(byte_pointer start, size_t len) {
    int i;
    for (i = 0; i < len; i++)
        printf(" %.2x", start[i]);
    printf("\n");
}

void show_int(int x) {
    show_bytes((byte_pointer) &x, sizeof(int));
}

void show_float(float x) {
    show_bytes((byte_pointer) &x, sizeof(float));
}

void show_pointer(void *x) {
    show_bytes((byte_pointer) &x, sizeof(void *));
}
```

过程`show_int`、`show_float`和`show_pointer`展示了如何使用程序`show_bytes`来分别输出类型为 `int`、`float` 和 `void*` 的 C 程序对象的字节表示。可以观察到它们仅仅传递给 `show_bytes`一个指向它们参数 x 的指针 \&x，且这个指针被强制类型转换为 `unsigned char*`。这种强制类型转换告诉编译器，程序应该把这个指针看成指向一个字节序列，而不是指向一个原始数据类型的对象。然后，这个指针会被看成是对象使用的最低字节地址。&#x20;

这些过程使用 C 语言的运算符 `sizeof` 来确定对象使用的字节数。一般来说，表达式 `sizeof(T)` 返回存储一个类型为 T 的对象所需要的字节数。使用 `sizeof` 而不是一个固定的值，是向编写在不同机器类型上可移植的代码迈进了一步。&#x20;

在几种不同的机器上运行下面所示的代码，得到如图 2-6 所示的结果。我们使用了以下几种机器： **Linux32**：运行 Linux 的 Intel IA32 处理器。 \
**Windows**：运行 Windows 的 Intel IA32 处理器。 \
**Sun**：运行 Solaris 的 Sun Microsystems SPARC处理器。（这些机器现在由 Oracle 生产。）\
**Linux64**：运行 Linux 的 Intel x86-64 处理器。

```c
void test_show_bytes(int val) {
    int ival = val;
    float fval = (float) ival;
    int *pval = &ival;
    show_int(ival);
    show_float(fval);
    show_pointer(pval);
}
```

![Figure 2.6 Byte representations of different data values. Results for int and float are identical, except for byte ordering. Pointer values are machine dependent.](<../../.gitbook/assets/image (4).png>)

参数 12345 的十六进制表示为 0x00003039。对于 `int` 类型的数据，除了字节顺序以外，我们在所有机器上都得到相同的结果。特别地，我们可以看到在 Linux32、Windows 和 Linux64 上，最低有效字节值0x39 最先输出，这说明它们是小端法机器；而在 Sun 上最后输出，这说明 Sun 是大端法机器。同样地，`float`数据的字节，除了字节顺序以外，也都是相同的。另一方面，指针值却是完全不同的。不同的机器/操作系统配置使用不同的存储分配规则。一个值得注意的特性是 Linux32、Windows 和 Sun 的机器使用 4 字节地址，而 Linux64 使用 8 字节地址。

可以观察到，尽管浮点型和整型数据都是对数值 12345 编码，但是它们有截然不同的字节模式：整型为0x00003039，而浮点数为 0×4640E400。一般而言，这两种格式使用不同的编码方法。如果我们将这些十六进制模式扩展为二进制形式，并且适当地将它们移位，就会发现一个有 13 个相匹配的位的序列，用一串星号标识出来：

![](<../../.gitbook/assets/image (3).png>)

这并不是巧合。当我们研究浮点数格式时，还将再回到这个例子。

{% tabs %}
{% tab title="Practice Problem 2.5" %}
Consider the following three calls to show\_bytes:&#x20;

int val = 0x87654321; \
byte\_pointer valp = (byte\_pointer) \&val; \
show\_bytes(valp, 1); /\* A. \*/\
show\_bytes(valp, 2); /\* B. \*/ \
show\_bytes(valp, 3); /\* C. \*/&#x20;

Indicate the values that will be printed by each call on a little-endian machine and on a big-endian machine:&#x20;

A. Little endian: __ Big endian:&#x20;

B. Little endian:  Big endian:&#x20;

C. Little endian:  Big endian:
{% endtab %}

{% tab title="Practice Problem 2.6" %}
Using `show_int` and `show_float`, we determine that the integer 3510593 has hexadecimal representation 0x00359141, while the floating-point number 3510593.0 has hexadecimal representation 0x4A564504.&#x20;

A. Write the binary representations of these two hexadecimal values.&#x20;

B. Shift these two strings relative to one another to maximize the number of matching bits. How many bits match?&#x20;

C. What parts of the strings do not match?
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Solution 2.5" %}
A.

Little endian: 21

Big endian: 87

B.

Little endian: 21 43

Big endian: 87 65

C.

Little endian: 21 43 65

Big endian: 87 65 43
{% endtab %}

{% tab title="Solution 2.6" %}
A.

0x00359141:  0000 0000 0011 0101 1001 0001 0100 0001

0x4A564504: 0100 1010 0101 0110 0100 0101 0000 0100

B.

0x00359141:  0000 0000 001**1 0101 1001 0001 0100 0001**

0x4A564504: 0100 1010 0**101 0110 0100 0101 0000 01**00

21 bits match most.

C.

as B shows.

For floating-point number, first 9 bits and last 2 bits not match; and for integer number, first 11 bits not match.
{% endtab %}
{% endtabs %}

## 2.1.4 表示字符串

C 语言中字符串被编码为一个以 null (其值为 0) 字符结尾的字符数组。每个字符都由某个标准编码来表示，最常见的是 ASCII 字符码。因此，如果我们以参数 "12345" 和 6 (包括终止符) 来运行例程 `show_bytes`，我们得到结果 31 32 33 34 35 00。请注意，十进制数字 x 的 ASCII 码正好是 0x3x，而终止字节的十六进制表示为 0x00。**在使用 ASCII 码作为字符码的任何系统上都将得到相同的结果，与字节顺序和字大小规则无关。因而，文本数据比二进制数据具有更强的平台独立性。**

{% tabs %}
{% tab title="Practice Problem 2.7" %}
What would be printed as a result of the following call to `show_bytes`?&#x20;

const char \*s = "abcdef";\
show\_bytes( (byte\_pointer) s, strlen(s) );

Note that letters ‘a’ through ‘z’ have ASCII codes 0x61 through 0x7A.
{% endtab %}
{% endtabs %}

{% tabs %}
{% tab title="Solution 2.7" %}
Result: 61 62 63 64 65 66

Because `strlen()` does not count null character.
{% endtab %}
{% endtabs %}

{% hint style="info" %}
#### 旁注 --- 文字编码的 Unicode 标准

ASCII 字符集适合于编码英语文档，但是在表达一些特殊字符方面并没有太多办法，例如法语的 ‘¸c’。它完全不适合编码希腊语、俄语和中文等语言的文档。这些年，提出了很多方法来对不同语言的文字进行编码。Unicode 联合会（Unicode Consortium）修订了最全面且广泛接受的文字编码标准。当前的 Unicode 标准（7.0 版）的字库包括将近 100 000 个字符，支持广泛的语言种类，包括古埃及和巴比伦的语言。为了保持信用，Unicode 技术委员会否决了为 Klingon（即电视连续剧《星际迷航》中的虚构文明）编写语言标准的提议。&#x20;

The base encoding，称为 Unicode 的 "Universal Character Set"，使用 32 位来表示字符。这好像要求文本串中每个字符要占用 4 个字节。不过，可以有一些替代编码，常见的字符只需要 1 个或 2 个字节，而不太常用的字符需要多一些的字节数。特别地，UTF-8 表示将每个字符编码为一个字节序列，这样标准 ASCII 字符还是使用和它们在 ASCII 中一样的单字节编码，这也就意味着所有的 ASCII 字节序列用 ASCII 码表示和用 UTF-8 表示是一样的。

Java 编程语言使用 Unicode 来表示字符串。对于 C 语言也有支持 Unicode 的程序库。
{% endhint %}

## 2.1.5 表示代码

考虑下面的 C 函数:

```c
int sum(int x, int y) {
    return x+y;
}
```

当我们在示例机器上编译时，生成如下字节表示的机器代码：

**Linux 32**     55 89 e5 8b 45 0c 03 45 08 c9 c3\
**Windows**   55 89 e5 8b 45 0c 03 45 08 5d c3\
**Sun**             81 c3 e0 08 90 02 00 09\
**Linux 64**    55 48 89 e5 89 7d fc 8975 f8 03 45 fc c9 c3

我们发现指令编码是不同的。不同的机器类型使用不同的且不兼容的指令和编码方式。即使是完全一样的进程，运行在不同的操作系统上也会有不同的编码规则，因此二进制代码是不兼容的。二进制代码很少能在不同机器和操作系统组合之间移植。&#x20;

计算机系统的一个基本概念就是，从机器的角度来看，程序仅仅只是字节序列。机器没有关于原始源程序的任何信息，除了可能有些用来帮助调试的辅助表以外。在第3章学习机器级编程时，我们将更清楚地看到这一点。

## 2.1.6 布尔代数简介

## 2.1.7 C 语言中的位级运算

## 2.1.8 C 语言中的逻辑运算

## 2.1.9 C 语言中的移位运算
